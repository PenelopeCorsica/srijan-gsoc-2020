{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Federated Learning - GTEx_V8 Example</h1>\n",
    "<h2>Populate remote PyGrid nodes with labeled tensors </h2>\n",
    "In this notebook, we will populate our PyGrid nodes with labeled data so that it will be used later by people interested in train models.\n",
    "\n",
    "**NOTE:** At the time of running this notebook, we were running the grid components in background mode.  \n",
    "\n",
    "Components:\n",
    " - PyGrid Network (http://localhost:5000)\n",
    " - PyGrid Node h1 (http://localhost:3000)\n",
    " - PyGrid Node h2 (http://localhost:3001)\n",
    " \n",
    "Code implementation for this notebook has been referred from <a href=\"https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/grid/federated_learning/mnist/Fed.Learning%20MNIST%20%5B%20Part-1%20%5D%20-%20Populate%20a%20Grid%20Network%20(%20Dataset%20).ipynb\">Fed.Learning MNIST [ Part-1 ] - Populate a Grid Network ( Dataset )</a> tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies for helper functions/classes\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from typing import NamedTuple\n",
    "import os.path as path\n",
    "import os\n",
    "import progressbar\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#keras for ML\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.utils import plot_model, normalize\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Nadam, Adadelta\n",
    "from tensorflow.keras.activations import relu, elu, sigmoid\n",
    "\n",
    "#sklearn for preprocessing the data and train-test split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "#for plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Parameter cell -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "dataset_size = 17000\n",
    "n_classes = 6\n",
    "no_samples_to_take = dataset_size // n_classes\n",
    "\n",
    "# Connect directly to grid nodes\n",
    "nodes = [\"ws://0.0.0.0:3000/\",\n",
    "         \"ws://0.0.0.0:3001/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "#########<syft==0.2.8>#######################\n",
    "# # Dynamic FL -->\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "\n",
    "# #Static FL -->\n",
    "from syft.grid.clients.model_centric_fl_client import ModelCentricFLClient\n",
    "#############################################\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Labels(NamedTuple):\n",
    "    '''\n",
    "    One-hot labeled data\n",
    "    '''\n",
    "    tissue: np.ndarray\n",
    "    sex: np.ndarray\n",
    "    age: np.ndarray\n",
    "    death: np.ndarray\n",
    "        \n",
    "\n",
    "class Genes:\n",
    "    '''\n",
    "    Class to load GTEX samples and gene expressions data\n",
    "    '''\n",
    "    def __init__(self, samples_path: str = '', expressions_path: str = '', problem_type: str = \"classification\"):\n",
    "        self.__set_samples(samples_path)\n",
    "        self.__set_labels(problem_type)\n",
    "        if expressions_path != '':\n",
    "            self.expressions = self.get_expressions(expressions_path)\n",
    "\n",
    "    def __set_samples(self, sample_path: str) -> pd.DataFrame:\n",
    "        self.samples: pd.DataFrame = pq.read_table(sample_path).to_pandas()\n",
    "        self.samples[\"Death\"].fillna(-1.0, inplace = True)\n",
    "        self.samples: pd.DataFrame = self.samples.set_index(\"Name\")\n",
    "        self.samples[\"Sex\"].replace([1, 2], ['male', 'female'], inplace=True)\n",
    "        self.samples[\"Death\"].replace([-1,0,1,2,3,4], ['alive/NA', 'ventilator case', '<10 min.', '<1 hr', '1-24 hr.', '>1 day'], inplace=True)\n",
    "        self.samples = self.samples[~self.samples['Death'].isin(['>1 day'])]\n",
    "        return self.samples\n",
    "\n",
    "    def __set_labels(self, problem_type: str = \"classification\") -> Labels:\n",
    "        self.labels_list = [\"Tissue\", \"Sex\", \"Age\", \"Death\"]\n",
    "        self.labels: pd.DataFrame = self.samples[self.labels_list]\n",
    "        self.drop_list = self.labels_list + [\"Subtissue\", \"Avg_age\"]\n",
    "        \n",
    "        if problem_type == \"classification\":\n",
    "            dummies_df = pd.get_dummies(self.labels[\"Age\"])\n",
    "            print(dummies_df.columns.tolist())\n",
    "            self.Y = dummies_df.values\n",
    "        \n",
    "        if problem_type == \"regression\":\n",
    "            self.Y = self.samples[\"Avg_age\"].values\n",
    "        \n",
    "        return self.Y\n",
    "\n",
    "    def sex_output(self, model):\n",
    "        return Dense(units=self.Y.sex.shape[1], activation='softmax', name='sex_output')(model)\n",
    "\n",
    "    def tissue_output(self, model):\n",
    "        return Dense(units=self.Y.tissue.shape[1], activation='softmax', name='tissue_output')(model)\n",
    "\n",
    "    def death_output(self, model):\n",
    "        return Dense(units=self.Y.death.shape[1], activation='softmax', name='death_output')(model)\n",
    "\n",
    "    def age_output(self, model):\n",
    "        '''\n",
    "        Created an output layer for the keras mode\n",
    "        :param model: keras model\n",
    "        :return: keras Dense layer\n",
    "        '''\n",
    "        return Dense(units=self.Y.age.shape[1], activation='softmax', name='age_output')(model)\n",
    "\n",
    "\n",
    "    def get_expressions(self, expressions_path: str)->pd.DataFrame:\n",
    "        '''\n",
    "        load gene expressions DataFrame\n",
    "        :param expressions_path: path to file with expressions\n",
    "        :return: pandas dataframe with expression\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if expressions_path.endswith(\".parquet\"):\n",
    "            return pq.read_table(expressions_path).to_pandas().set_index(\"Name\") \n",
    "        else:\n",
    "            separator = \",\" if expressions_path.endswith(\".csv\") else \"\\t\"\n",
    "            return pd.read_csv(expressions_path, sep=separator).set_index(\"Name\") \n",
    "\n",
    "    def prepare_data(self, normalize_expressions: bool = True)-> np.ndarray:\n",
    "        '''\n",
    "        :param normalize_expressions: if keras should normalize gene expressions\n",
    "        :return: X array to be used as input data by keras\n",
    "        '''\n",
    "        data = self.samples.join(self.expressions, on = \"Name\", how=\"inner\")\n",
    "        ji = data.columns.drop(self.drop_list)\n",
    "        x = data[ji]\n",
    "        \n",
    "        # adding one-hot-encoded tissues and sex\n",
    "        #x = pd.concat([x,pd.get_dummies(data['Tissue'], prefix='tissue'), pd.get_dummies(data['Sex'], prefix='sex')],axis=1)\n",
    "        \n",
    "        steps = [('standardization', StandardScaler()), ('normalization', MinMaxScaler())]\n",
    "        pre_processing_pipeline = Pipeline(steps)\n",
    "        transformed_data = pre_processing_pipeline.fit_transform(x)\n",
    "\n",
    "        x = transformed_data\n",
    "        \n",
    "        print('Data length', len(x))\n",
    "        \n",
    "        return x #normalize(x, axis=0) if normalize_expressions else x\n",
    "    \n",
    "    def get_features_dataframe(self, add_tissues=False):\n",
    "        data = self.samples.join(self.expressions, on = \"Name\", how=\"inner\")\n",
    "        ji = data.columns.drop(self.drop_list)\n",
    "        df = data[ji]\n",
    "        if add_tissues:\n",
    "            df = pd.concat([df,pd.get_dummies(data['Tissue'], prefix='tissue'), pd.get_dummies(data['Sex'], prefix='sex')],axis=1)\n",
    "        x = df.values\n",
    "        \n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(x)\n",
    "        df_normalized = pd.DataFrame(x_scaled, columns=df.columns, index=df.index)\n",
    "        return df_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_path = '../data/gtex/v8_samples.parquet'\n",
    "expressions_path = '../data/gtex/v8_expressions.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length 15343\n"
     ]
    }
   ],
   "source": [
    "genes = Genes(samples_path, expressions_path, problem_type=\"regression\")\n",
    "X = genes.prepare_data(True)\n",
    "Y = genes.Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_1, X_2, y_1, y_2 = model_selection.train_test_split(X, Y, test_size=0.5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dtype = np.float32\n",
    "y_1 = np.vstack(y_1).astype(np.float32)\n",
    "y_2 = np.vstack(y_2).astype(np.float32)\n",
    "X_1.dtype, y_1.dtype,X_2.dtype, y_2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.8'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setup config</h2>\n",
    "Init hook, connect with grid nodes, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "compute_nodes = []\n",
    "for node in nodes:\n",
    "    # For syft 0.2.8 --> replace DynamicFLClient with DataCentricFLClient\n",
    "    compute_nodes.append( DataCentricFLClient(hook, node) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Federated Worker id:h1>, <Federated Worker id:h2>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Conversion to Tensor\n",
    "\n",
    "The code below will convert GTEx data samples to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajverma/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook.py:560: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_tensor = hook_self.torch.native_tensor(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "shared_x1, shared_x2, shared_y1, shared_y2 = X_1, X_2, y_1, y_2\n",
    "\n",
    "# Convert numpy array to torch tensors -->\n",
    "shared_x1 = torch.from_numpy(shared_x1)\n",
    "shared_x2 = torch.from_numpy(shared_x2)\n",
    "shared_y1 = torch.from_numpy(shared_y1)\n",
    "shared_y2 = torch.from_numpy(shared_y2)\n",
    "\n",
    "shared_x1 = torch.tensor(shared_x1, dtype=torch.float32)\n",
    "shared_x2 = torch.tensor(shared_x2, dtype=torch.float32)\n",
    "shared_y1 = torch.tensor(shared_y1, dtype=torch.float32)\n",
    "shared_y2 = torch.tensor(shared_y2, dtype=torch.float32)\n",
    "\n",
    "datasets  = [shared_x1, shared_x2]\n",
    "labels = [shared_y1, shared_y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0149, 0.0696, 0.2304,  ..., 0.1199, 0.0991, 0.1440]),\n",
       " tensor([64.5000]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_x1[0], shared_y2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate \n",
    "X = torch.cat((shared_x1, shared_x2), dim=0)\n",
    "Y = torch.cat((shared_y1, shared_y2), dim=0)\n",
    "rmse = []\n",
    "mae = []\n",
    "r2 = []\n",
    "huber_loss = []\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# TODO: Define your network architecture here\n",
    "class Regression(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "        self.dropout = nn.Dropout(0.1) \n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create the network, define the criterion and optimizer\n",
    "NUM_FEATURES = 18388\n",
    "model = Regression(NUM_FEATURES)\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below training in a federated way, but without sending data to nodes (full data) --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 | With h1 data |: Train Loss: 2892.307 | R^2: -16.854 | Mean squared error: 2892.307 | Mean absolute error: 52.252\n",
      "Train Epoch: 0 | With h2 data |: Train Loss: 2874.802 | R^2: -16.427 | Mean squared error: 2874.802 | Mean absolute error: 52.055\n",
      "Train Epoch: 1 | With h1 data |: Train Loss: 2859.383 | R^2: -16.650 | Mean squared error: 2859.383 | Mean absolute error: 51.934\n",
      "Train Epoch: 1 | With h2 data |: Train Loss: 2834.686 | R^2: -16.184 | Mean squared error: 2834.687 | Mean absolute error: 51.666\n"
     ]
    }
   ],
   "source": [
    "def epoch_total_size(data):\n",
    "    total = 0\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            total += data[i][j].shape[0]\n",
    "            \n",
    "    return total\n",
    "\n",
    "data = datasets\n",
    "target = labels\n",
    "opt = optimizer\n",
    "N_EPOCS = 2\n",
    "rmse = []\n",
    "mae = []\n",
    "r2 = []\n",
    "huber_loss = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    epoch_total = epoch_total_size(data)\n",
    "    current_epoch_size = 0\n",
    "    for i in range(len(data)):\n",
    "        correct = 0\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        epoch_acc = 0.0\n",
    "        current_epoch_size += len(data[i])\n",
    "\n",
    "        opt.zero_grad()\n",
    "        pred = model(data[i])\n",
    "        loss = loss_func(pred, target[i])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        y_pred = pred.detach().numpy()\n",
    "        y_true = target[i].detach().numpy()\n",
    "\n",
    "        rmse.append(mean_squared_error(y_true, y_pred))\n",
    "        mae.append(mean_absolute_error(y_true, y_pred))\n",
    "        r2.append(r2_score(y_true, y_pred))\n",
    "\n",
    "        print('Train Epoch: {} | With h{} data |: Train Loss: {:.3f} | R^2: {:.3f} | Mean squared error: {:.3f} | Mean absolute error: {:.3f}'.format(\n",
    "                  epoch, i+1, loss, r2_score(y_true, y_pred), mean_squared_error(y_true, y_pred), mean_absolute_error(y_true, y_pred)))\n",
    "\n",
    "for epoch in range(N_EPOCS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2 - Tagging tensors</h2>\n",
    "The code below will add a tag (of your choice) to the data that will be sent to grid nodes. This tag is important as the network will need it to retrieve this data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = [datasets[0][0:200], datasets[1][0:200]]\n",
    "dummy_label = [labels[0][0:200], labels[1][0:200]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0149, 0.0696, 0.2304,  ..., 0.1199, 0.0991, 0.1440],\n",
       "         [0.0000, 0.0322, 0.1970,  ..., 0.0884, 0.0457, 0.2258],\n",
       "         [0.0142, 0.0029, 0.1234,  ..., 0.3109, 0.1795, 0.4097],\n",
       "         ...,\n",
       "         [0.0352, 0.0050, 0.2004,  ..., 0.2331, 0.1092, 0.3676],\n",
       "         [0.0825, 0.0218, 0.4244,  ..., 0.1037, 0.0431, 0.2743],\n",
       "         [0.0142, 0.0307, 0.2735,  ..., 0.1049, 0.0498, 0.1756]]),\n",
       " tensor([[0.0085, 0.0005, 0.0511,  ..., 0.0172, 0.0095, 0.0196],\n",
       "         [0.0271, 0.0364, 0.0961,  ..., 0.3226, 0.1927, 0.4173],\n",
       "         [0.0474, 0.0038, 0.3967,  ..., 0.3229, 0.2227, 0.1453],\n",
       "         ...,\n",
       "         [0.1076, 0.0026, 0.2818,  ..., 0.1928, 0.0770, 0.2906],\n",
       "         [0.0262, 0.0849, 0.2029,  ..., 0.0505, 0.0225, 0.1307],\n",
       "         [0.0277, 0.0046, 0.1387,  ..., 0.1874, 0.1068, 0.3855]])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_input = []\n",
    "tag_label = []\n",
    "\n",
    "for i in range(len(compute_nodes)):\n",
    "    tag_input.append(dummy_data[i].tag(\"#X\", \"#gtex_v8\", \"#dataset\",\"#balanced\").describe(\"The input datapoints to the GTEx_V8 dataset.\"))\n",
    "    tag_label.append(dummy_label[i].tag(\"#Y\", \"#gtex_v8\", \"#dataset\",\"#balanced\").describe(\"The input labels to the GTEx_V8 dataset.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3 - Sending our tensors to grid nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebSocketConnectionClosedException",
     "evalue": "Connection is already closed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebSocketConnectionClosedException\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-9d036067df03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshared_x1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# First chunk of dataset to h1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshared_x2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Second chunk of dataset to h2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshared_y1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# First chunk of labels to h1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshared_y2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Second chunk of labels to h2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/generic/abstract/hookable.py\u001b[0m in \u001b[0;36mhooked_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhooked_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mmap_chain_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_before_{method_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mreturn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhookable_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mreturn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_chain_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_after_{method_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, inplace, user, local_autograd, requires_grad, preinitialize_grad, no_wrap, garbage_collect_data, *location)\u001b[0m\n\u001b[1;32m    492\u001b[0m                 \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0mpreinitialize_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreinitialize_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mgarbage_collect_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgarbage_collect_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m             )\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj, workers, ptr_id, garbage_collect_data, requires_grad, create_pointer, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# Send the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_obj\u001b[0;34m(self, obj, location)\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mreceive\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \"\"\"\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjectMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     def request_obj(\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_pending_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/workers/websocket_client.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m\"\"\"Forwards a message to the WebsocketServerWorker\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_to_websocket_server_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Websocket connection closed (worker: %s)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/grid/clients/data_centric_fl_client.py\u001b[0m in \u001b[0;36m_forward_to_websocket_server_worker\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mopcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mABNF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPCODE_TEXT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_data\u001b[0;34m(self, control_frame)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mreturn\u001b[0m  \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrol_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_data_frame\u001b[0;34m(self, control_frame)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;31m# handle error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36mrecv_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mABNF\u001b[0m \u001b[0mframe\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \"\"\"\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTATUS_NORMAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_received_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsv3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_header\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_abnf.py\u001b[0m in \u001b[0;36mrecv_strict\u001b[0;34m(self, bufsize)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# buffers allocated and then shrunk, which results in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# fragmentation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0mbytes_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mshortage\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_core.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, bufsize)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mWebSocketConnectionClosedException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/websocket/_socket.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(sock, bufsize)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbytes_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         raise WebSocketConnectionClosedException(\n\u001b[0;32m--> 115\u001b[0;31m             \"Connection is already closed.\")\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebSocketConnectionClosedException\u001b[0m: Connection is already closed."
     ]
    }
   ],
   "source": [
    "shared_x1 = tag_input[0].send(compute_nodes[0]) # First chunk of dataset to h1\n",
    "shared_x2 = tag_input[1].send(compute_nodes[1]) # Second chunk of dataset to h2\n",
    "\n",
    "shared_y1 = tag_label[0].send(compute_nodes[0]) # First chunk of labels to h1\n",
    "shared_y2 = tag_label[1].send(compute_nodes[1]) # Second chunk of labels to h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor pointers:  (Wrapper)>[PointerTensor | me:70361333315 -> h1:35881159115]\n",
      "\tTags: #balanced #dataset #gtex_v8 #X \n",
      "\tShape: torch.Size([900, 18420])\n",
      "\tDescription: The input datapoints to the GTEx_V8 dataset.... (Wrapper)>[PointerTensor | me:49684275468 -> h2:52723085225]\n",
      "\tTags: #balanced #dataset #gtex_v8 #X \n",
      "\tShape: torch.Size([900, 18420])\n",
      "\tDescription: The input datapoints to the GTEx_V8 dataset....\n",
      "Y tensor pointers:  (Wrapper)>[PointerTensor | me:49137384384 -> h1:4180671457]\n",
      "\tTags: #balanced #Y #gtex_v8 #dataset \n",
      "\tShape: torch.Size([900])\n",
      "\tDescription: The input labels to the GTEx_V8 dataset.... (Wrapper)>[PointerTensor | me:10561419581 -> h2:1629165329]\n",
      "\tTags: #balanced #Y #gtex_v8 #dataset \n",
      "\tShape: torch.Size([900])\n",
      "\tDescription: The input labels to the GTEx_V8 dataset....\n"
     ]
    }
   ],
   "source": [
    "print(\"X tensor pointers: \", shared_x1, shared_x2)\n",
    "print(\"Y tensor pointers: \", shared_y1, shared_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:9001755644 -> h1:12992579137]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_x1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Disconnect nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(compute_nodes)):\n",
    "    compute_nodes[i].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to the following address to search available tags:\n",
    "http://0.0.0.0:5000/search-available-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysyft_v028] *",
   "language": "python",
   "name": "conda-env-pysyft_v028-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
