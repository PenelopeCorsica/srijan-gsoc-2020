{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Federated Learning - GTEx_V8 Example</h1>\n",
    "<h2>Populate remote PyGrid nodes with labeled tensors </h2>\n",
    "In this notebook, we will populate our PyGrid nodes with labeled data so that it will be used later by people interested in train models.\n",
    "\n",
    "**NOTE:** At the time of running this notebook, we were running the grid components in background mode.  \n",
    "\n",
    "Components:\n",
    " - PyGrid Network (http://localhost:5000)\n",
    " - PyGrid Node h1 (http://localhost:3000)\n",
    " - PyGrid Node h2 (http://localhost:3001)\n",
    " \n",
    "Code implementation for this notebook has been referred from <a href=\"https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/grid/federated_learning/mnist/Fed.Learning%20MNIST%20%5B%20Part-1%20%5D%20-%20Populate%20a%20Grid%20Network%20(%20Dataset%20).ipynb\">Fed.Learning MNIST [ Part-1 ] - Populate a Grid Network ( Dataset )</a> tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies for helper functions/classes\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from typing import NamedTuple\n",
    "from dataclasses import *\n",
    "import os.path as path\n",
    "import os\n",
    "import progressbar\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "#keras for ML\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.utils import plot_model, normalize\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Nadam, Adadelta\n",
    "from tensorflow.keras.activations import relu, elu, sigmoid\n",
    "\n",
    "#sklearn for preprocessing the data and train-test split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "#for plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "seed = 7\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "#########<syft==0.2.8>#######################\n",
    "# # Dynamic FL -->\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "\n",
    "# #Static FL -->\n",
    "from syft.grid.clients.model_centric_fl_client import ModelCentricFLClient\n",
    "#############################################\n",
    "\n",
    "#########<syft==0.2.6>#######################\n",
    "# Dynamic FL -->\n",
    "# from syft.grid.clients.dynamic_fl_client import DynamicFLClient\n",
    "\n",
    "#Static FL -->\n",
    "# from syft.grid.clients.static_fl_client import StaticFLClient\n",
    "#############################################\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "# import tqdm\n",
    "# from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Labels(NamedTuple):\n",
    "    '''\n",
    "    One-hot labeled data\n",
    "    '''\n",
    "    tissue: np.ndarray\n",
    "    sex: np.ndarray\n",
    "    age: np.ndarray\n",
    "    death: np.ndarray\n",
    "        \n",
    "\n",
    "class Genes:\n",
    "    '''\n",
    "    Class to load GTEX samples and gene expressions data\n",
    "    '''\n",
    "    def __init__(self, samples_path: str = '', expressions_path: str = '', problem_type: str = \"classification\"):\n",
    "        self.__set_samples(samples_path)\n",
    "        self.__set_labels(problem_type)\n",
    "        if expressions_path != '':\n",
    "            self.expressions = self.get_expressions(expressions_path)\n",
    "\n",
    "    def __set_samples(self, sample_path: str) -> pd.DataFrame:\n",
    "        self.samples: pd.DataFrame = pq.read_table(sample_path).to_pandas()\n",
    "        self.samples[\"Death\"].fillna(-1.0, inplace = True)\n",
    "        self.samples: pd.DataFrame = self.samples.set_index(\"Name\")\n",
    "        self.samples[\"Sex\"].replace([1, 2], ['male', 'female'], inplace=True)\n",
    "        self.samples[\"Death\"].replace([-1,0,1,2,3,4], ['alive/NA', 'ventilator case', '<10 min.', '<1 hr', '1-24 hr.', '>1 day'], inplace=True)\n",
    "    \n",
    "        return self.samples\n",
    "\n",
    "    def __set_labels(self, problem_type: str = \"classification\") -> Labels:\n",
    "        self.labels_list = [\"Tissue\", \"Sex\", \"Age\", \"Death\"]\n",
    "        self.labels: pd.DataFrame = self.samples[self.labels_list]\n",
    "        self.drop_list = self.labels_list + [\"Subtissue\", \"Avg_age\"]\n",
    "        \n",
    "        if problem_type == \"classification\":\n",
    "            dummies_df = pd.get_dummies(self.labels[\"Age\"])\n",
    "            print(dummies_df.columns.tolist())\n",
    "            self.Y = dummies_df.values\n",
    "        \n",
    "        if problem_type == \"regression\":\n",
    "            self.Y = self.samples[\"Avg_age\"].values\n",
    "        \n",
    "        return self.Y\n",
    "    \n",
    "    def delete_particular_age_examples(self):\n",
    "        df_series = pd.DataFrame(self.labels[\"Age\"])\n",
    "        indexes_of_50 = np.where(df_series[\"Age\"] == '50-59')[0].tolist()[300:]\n",
    "        indexes_of_60 = np.where(df_series[\"Age\"] == '60-69')[0].tolist()[300:]\n",
    "        indexes_of_20 = np.where(df_series[\"Age\"] == '20-29')[0].tolist()[300:]\n",
    "        indexes_of_30 = np.where(df_series[\"Age\"] == '30-39')[0].tolist()[300:]\n",
    "        indexes_of_40 = np.where(df_series[\"Age\"] == '40-49')[0].tolist()[300:]\n",
    "        indexes_to_delete = indexes_of_50 + indexes_of_60 + indexes_of_20 + indexes_of_30 + indexes_of_40\n",
    "        \n",
    "        return indexes_to_delete\n",
    "\n",
    "    def sex_output(self, model):\n",
    "        return Dense(units=self.Y.sex.shape[1], activation='softmax', name='sex_output')(model)\n",
    "\n",
    "    def tissue_output(self, model):\n",
    "        return Dense(units=self.Y.tissue.shape[1], activation='softmax', name='tissue_output')(model)\n",
    "\n",
    "    def death_output(self, model):\n",
    "        return Dense(units=self.Y.death.shape[1], activation='softmax', name='death_output')(model)\n",
    "\n",
    "    def age_output(self, model):\n",
    "        '''\n",
    "        Created an output layer for the keras mode\n",
    "        :param model: keras model\n",
    "        :return: keras Dense layer\n",
    "        '''\n",
    "        return Dense(units=self.Y.age.shape[1], activation='softmax', name='age_output')(model)\n",
    "\n",
    "\n",
    "    def get_expressions(self, expressions_path: str)->pd.DataFrame:\n",
    "        '''\n",
    "        load gene expressions DataFrame\n",
    "        :param expressions_path: path to file with expressions\n",
    "        :return: pandas dataframe with expression\n",
    "        '''\n",
    "        if expressions_path.endswith(\".parquet\"):\n",
    "            return pq.read_table(expressions_path).to_pandas().set_index(\"Name\")\n",
    "        else:\n",
    "            separator = \",\" if expressions_path.endswith(\".csv\") else \"\\t\"\n",
    "            return pd.read_csv(expressions_path, sep=separator).set_index(\"Name\")\n",
    "\n",
    "    def prepare_data(self, normalize_expressions: bool = True)-> np.ndarray:\n",
    "        '''\n",
    "        :param normalize_expressions: if keras should normalize gene expressions\n",
    "        :return: X array to be used as input data by keras\n",
    "        '''\n",
    "        data = self.samples.join(self.expressions, on = \"Name\", how=\"inner\")\n",
    "        ji = data.columns.drop(self.drop_list)\n",
    "        x = data[ji]\n",
    "        \n",
    "        # adding one-hot-encoded tissues and sex\n",
    "        x = pd.concat([x,pd.get_dummies(data['Tissue'], prefix='tissue'), pd.get_dummies(data['Sex'], prefix='sex')],axis=1)\n",
    "        x = x.values\n",
    "        \n",
    "        return normalize(x, axis=0) if normalize_expressions else x\n",
    "    \n",
    "    def get_features_dataframe(self, add_tissues=True):\n",
    "        data = self.samples.join(self.expressions, on = \"Name\", how=\"inner\")\n",
    "        ji = data.columns.drop(self.drop_list)\n",
    "        df = data[ji]\n",
    "        if add_tissues:\n",
    "            df = pd.concat([df,pd.get_dummies(data['Tissue'], prefix='tissue'), pd.get_dummies(data['Sex'], prefix='sex')],axis=1)\n",
    "        x = df.values\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(x)\n",
    "        df_normalized = pd.DataFrame(x_scaled, columns=df.columns, index=df.index)\n",
    "        return df_normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# resolving base folder\n",
    "wd = Path(\".\").resolve()\n",
    "base = wd if (wd / \"data\").exists() else Path(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = base / \"data\"\n",
    "samples_path = str(data / 'gtex' / 'v8_samples.parquet')\n",
    "expressions_path = str(data / 'gtex' / 'v8_expressions.parquet')\n",
    "\n",
    "# samples_path = '../data/gtex/v8_samples.parquet'\n",
    "# expressions_path = '../data/gtex/v8_expressions.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Huber(yHat, y, delta=1.):\n",
    "    return np.where(np.abs(y-yHat) < delta,.5*(y-yHat)**2 , delta*(np.abs(y-yHat)-0.5*delta))\n",
    "\n",
    "def transform_to_probas(age_intervals):\n",
    "    class_names = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n",
    "    res = []\n",
    "    for a in age_intervals:\n",
    "        non_zero_index = class_names.index(a)\n",
    "        res.append([0 if i != non_zero_index else 1 for i in range(len(class_names))])\n",
    "    return np.array(res)\n",
    "    \n",
    "def transform_to_interval(age_probas):\n",
    "    class_names = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n",
    "    return np.array(list(map(lambda p: class_names[np.argmax(p)], age_probas)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n"
     ]
    }
   ],
   "source": [
    "genes = Genes(samples_path, expressions_path, problem_type=\"classification\")\n",
    "X = genes.get_features_dataframe().values\n",
    "Y = genes.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17382, 18420), (17382, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20-29': 1320,\n",
       " '30-39': 1323,\n",
       " '40-49': 2702,\n",
       " '50-59': 5615,\n",
       " '60-69': 5821,\n",
       " '70-79': 601}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = transform_to_interval(Y)\n",
    "unique, counts = np.unique(a, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1320, 1: 1323, 2: 2702, 3: 5615, 4: 5821, 5: 601}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [np.where(r==1)[0][0] for r in Y]\n",
    "unique, counts = np.unique(b, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pie = pd.DataFrame(columns = ['age_group','label'])\n",
    "df_pie['age_group'] = a\n",
    "df_pie['label'] = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def balanced_sample_maker(X, y, sample_size, random_seed=None):\n",
    "    \"\"\" return a balanced data set by sampling all classes with sample_size \n",
    "        current version is developed on assumption that the positive\n",
    "        class is the minority.\n",
    "\n",
    "    Parameters:\n",
    "    ===========\n",
    "    X: {numpy.ndarrray}\n",
    "    y: {numpy.ndarray}\n",
    "    \"\"\"\n",
    "    uniq_levels = np.unique(y)\n",
    "    uniq_counts = {level: sum(y == level) for level in uniq_levels}\n",
    "\n",
    "    if not random_seed is None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # find observation index of each class levels\n",
    "    groupby_levels = {}\n",
    "    for ii, level in enumerate(uniq_levels):\n",
    "        obs_idx = [idx for idx, val in enumerate(y) if val == level]\n",
    "        groupby_levels[level] = obs_idx\n",
    "    # oversampling on observations of each label\n",
    "    balanced_copy_idx = []\n",
    "    for gb_level, gb_idx in groupby_levels.items():\n",
    "        over_sample_idx = np.random.choice(gb_idx, size=sample_size, replace=True).tolist()\n",
    "        balanced_copy_idx+=over_sample_idx\n",
    "    np.random.shuffle(balanced_copy_idx)\n",
    "\n",
    "    return (X[balanced_copy_idx, :], y[balanced_copy_idx], balanced_copy_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples_to_take = 200\n",
    "res = balanced_sample_maker(X,np.asarray(df_pie['age_group'].values), no_samples_to_take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20-29': 200,\n",
       " '30-39': 200,\n",
       " '40-49': 200,\n",
       " '50-59': 200,\n",
       " '60-69': 200,\n",
       " '70-79': 200}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(res[1], return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(res[1])\n",
    "y = le.transform(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into different shards --> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_1, X_2, y_1, y_2 = model_selection.train_test_split(res[0], y, test_size=0.5, random_state=seed, stratify=res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('uint8'), dtype('float32'), dtype('uint8'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dtype = np.float32\n",
    "y_1 = np.vstack(y_1).astype(np.uint8)\n",
    "y_2 = np.vstack(y_2).astype(np.uint8)\n",
    "X_1.dtype, y_1.dtype,X_2.dtype, y_2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100}\n",
      "{0: 100, 1: 100, 2: 100, 3: 100, 4: 100, 5: 100}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_1, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(y_2, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]\n",
    "y_1 = y_1.reshape(y_1.shape[0])\n",
    "y_2 = y_2.reshape(y_2.shape[0])\n",
    "y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.8'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setup config</h2>\n",
    "Init hook, connect with grid nodes, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "# Connect directly to grid nodes\n",
    "nodes = [\"ws://0.0.0.0:3000/\",\n",
    "         \"ws://0.0.0.0:3001/\"]\n",
    "\n",
    "compute_nodes = []\n",
    "for node in nodes:\n",
    "    # For syft 0.2.8 --> replace DynamicFLClient with DataCentricFLClient\n",
    "    compute_nodes.append( DataCentricFLClient(hook, node) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Federated Worker id:h1>, <Federated Worker id:h2>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Conversion to Tensor\n",
    "\n",
    "The code below will convert GTEx data samples to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajverma/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook.py:560: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_tensor = hook_self.torch.native_tensor(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# DATA_PATH = '../data/balanced/numpy_files/'\n",
    "# shared_x1 = np.load(DATA_PATH + 'shared_x1.npy') # First chunk of dataset \n",
    "# shared_x2 = np.load(DATA_PATH + 'shared_x2.npy') # Second chunk of dataset \n",
    "\n",
    "# shared_y1 = np.load(DATA_PATH + 'shared_y1.npy') # First chunk of labels \n",
    "# shared_y2 = np.load(DATA_PATH + 'shared_y2.npy') # Second chunk of labels \n",
    "\n",
    "shared_x1, shared_x2, shared_y1, shared_y2 = X_1, X_2, y_1, y_2\n",
    "\n",
    "# Convert numpy array to torch tensors -->\n",
    "shared_x1 = torch.from_numpy(shared_x1)\n",
    "shared_x2 = torch.from_numpy(shared_x2)\n",
    "shared_y1 = torch.from_numpy(shared_y1)\n",
    "shared_y2 = torch.from_numpy(shared_y2)\n",
    "\n",
    "shared_x1 = torch.tensor(shared_x1, dtype=torch.float32)\n",
    "shared_x2 = torch.tensor(shared_x2, dtype=torch.float32)\n",
    "shared_y1 = torch.tensor(shared_y1, dtype=torch.int64)\n",
    "shared_y2 = torch.tensor(shared_y2, dtype=torch.int64)\n",
    "\n",
    "datasets  = [shared_x1, shared_x2]\n",
    "labels = [shared_y1, shared_y2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below using centralized way (full data) --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.0014931774139404297  | Training Accuracy: 0.15666666666666668\n",
      "Epoch: 1 Training loss: 0.0014906399448712667  | Training Accuracy: 0.16666666666666666\n",
      "Epoch: 2 Training loss: 0.0014876883228619893  | Training Accuracy: 0.17\n",
      "Epoch: 3 Training loss: 0.001481928825378418  | Training Accuracy: 0.22166666666666668\n",
      "Epoch: 4 Training loss: 0.0014999624093373616  | Training Accuracy: 0.1975\n",
      "Epoch: 5 Training loss: 0.0015007805824279784  | Training Accuracy: 0.18\n",
      "Epoch: 6 Training loss: 0.0014997453490893046  | Training Accuracy: 0.1875\n",
      "Epoch: 7 Training loss: 0.0014792474110921223  | Training Accuracy: 0.22333333333333333\n",
      "Epoch: 8 Training loss: 0.0014908491571744282  | Training Accuracy: 0.1875\n",
      "Epoch: 9 Training loss: 0.0014751755197842916  | Training Accuracy: 0.21833333333333332\n",
      "Epoch: 10 Training loss: 0.0014737178881963095  | Training Accuracy: 0.2275\n",
      "Epoch: 11 Training loss: 0.0014729687571525573  | Training Accuracy: 0.24\n",
      "Epoch: 12 Training loss: 0.0014643908540407818  | Training Accuracy: 0.25666666666666665\n",
      "Epoch: 13 Training loss: 0.0014581912755966186  | Training Accuracy: 0.2675\n",
      "Epoch: 14 Training loss: 0.0014598410328229268  | Training Accuracy: 0.23916666666666667\n",
      "Epoch: 15 Training loss: 0.0014520078897476197  | Training Accuracy: 0.2683333333333333\n",
      "Epoch: 16 Training loss: 0.0014483241240183513  | Training Accuracy: 0.2925\n",
      "Epoch: 17 Training loss: 0.0014453664422035216  | Training Accuracy: 0.2966666666666667\n",
      "Epoch: 18 Training loss: 0.0014376996954282124  | Training Accuracy: 0.30666666666666664\n",
      "Epoch: 19 Training loss: 0.0014372416337331136  | Training Accuracy: 0.285\n"
     ]
    }
   ],
   "source": [
    "# Concatenate \n",
    "X = torch.cat((shared_x1, shared_x2), dim=0)\n",
    "Y = torch.cat((shared_y1, shared_y2), dim=0)\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TODO: Define your network architecture here\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(18420, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create the network, define the criterion and optimizer\n",
    "model = Classifier()\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "    \n",
    "epochs = 20\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss = F.cross_entropy(pred, Y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # statistics\n",
    "    #prob = F.softmax(pred, dim=1)\n",
    "    top1 = torch.argmax(pred, dim=1)\n",
    "    ncorrect = torch.sum(top1 == Y)\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_acc += ncorrect.item()\n",
    "\n",
    "    epoch_loss /= Y.shape[0]\n",
    "    epoch_acc /= Y.shape[0]\n",
    "\n",
    "    print(f\"Epoch: {e}\",f\"Training loss: {epoch_loss}\", f\" | Training Accuracy: {epoch_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2 - Tagging tensors</h2>\n",
    "The code below will add a tag (of your choice) to the data that will be sent to grid nodes. This tag is important as the network will need it to retrieve this data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_input = []\n",
    "tag_label = []\n",
    "\n",
    "for i in range(len(compute_nodes)):\n",
    "    tag_input.append(datasets[i].tag(\"#X\", \"#gtex_v8\", \"#dataset\",\"#balanced\").describe(\"The input datapoints to the GTEx_V8 dataset.\"))\n",
    "    tag_label.append(labels[i].tag(\"#Y\", \"#gtex_v8\", \"#dataset\",\"#balanced\").describe(\"The input labels to the GTEx_V8 dataset.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3 - Sending our tensors to grid nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_x1 = tag_input[0].send(compute_nodes[0]) # First chunk of dataset to h1\n",
    "shared_x2 = tag_input[1].send(compute_nodes[1]) # Second chunk of dataset to h2\n",
    "\n",
    "shared_y1 = tag_label[0].send(compute_nodes[0]) # First chunk of labels to h1\n",
    "shared_y2 = tag_label[1].send(compute_nodes[1]) # Second chunk of labels to h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor pointers:  (Wrapper)>[PointerTensor | me:28619960005 -> h1:40335857618]\n",
      "\tTags: #gtex_v8 #X #balanced #dataset \n",
      "\tShape: torch.Size([600, 18420])\n",
      "\tDescription: The input datapoints to the GTEx_V8 dataset.... (Wrapper)>[PointerTensor | me:64079951987 -> h2:67976316706]\n",
      "\tTags: #gtex_v8 #X #balanced #dataset \n",
      "\tShape: torch.Size([600, 18420])\n",
      "\tDescription: The input datapoints to the GTEx_V8 dataset....\n",
      "Y tensor pointers:  (Wrapper)>[PointerTensor | me:83086150454 -> h1:76247224541]\n",
      "\tTags: #Y #gtex_v8 #balanced #dataset \n",
      "\tShape: torch.Size([600])\n",
      "\tDescription: The input labels to the GTEx_V8 dataset.... (Wrapper)>[PointerTensor | me:44335625920 -> h2:69428496864]\n",
      "\tTags: #Y #gtex_v8 #balanced #dataset \n",
      "\tShape: torch.Size([600])\n",
      "\tDescription: The input labels to the GTEx_V8 dataset....\n"
     ]
    }
   ],
   "source": [
    "print(\"X tensor pointers: \", shared_x1, shared_x2)\n",
    "print(\"Y tensor pointers: \", shared_y1, shared_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Disconnect nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(compute_nodes)):\n",
    "    compute_nodes[i].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to the following address to search available tags:\n",
    "http://0.0.0.0:5000/search-available-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysyft_v028] *",
   "language": "python",
   "name": "conda-env-pysyft_v028-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
