{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Federated Learning - GTEx_V8 Example</h1>\n",
    "<h2>Populate remote PyGrid nodes with labeled tensors </h2>\n",
    "In this notebook, we will populate our PyGrid nodes with labeled data so that it will be used later by people interested in train models.\n",
    "\n",
    "**NOTE:** At the time of running this notebook, we were running the grid components in background mode.  \n",
    "\n",
    "Components:\n",
    " - PyGrid Network (http://localhost:5000)\n",
    " - PyGrid Node h1 (http://localhost:3000)\n",
    " - PyGrid Node h2 (http://localhost:3001)\n",
    " \n",
    "Code implementation for this notebook has been referred from <a href=\"https://github.com/OpenMined/PySyft/blob/master/examples/tutorials/grid/federated_learning/mnist/Fed.Learning%20MNIST%20%5B%20Part-1%20%5D%20-%20Populate%20a%20Grid%20Network%20(%20Dataset%20).ipynb\">Fed.Learning MNIST [ Part-1 ] - Populate a Grid Network ( Dataset )</a> tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies for helper functions/classes\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from typing import NamedTuple\n",
    "from dataclasses import *\n",
    "import os.path as path\n",
    "import os\n",
    "import progressbar\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "#keras for ML\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.utils import plot_model, normalize\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Nadam, Adadelta\n",
    "from tensorflow.keras.activations import relu, elu, sigmoid\n",
    "\n",
    "#sklearn for preprocessing the data and train-test split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "#for plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Parameter cell -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "dataset_size = 1800\n",
    "n_classes = 6\n",
    "no_samples_to_take = dataset_size // n_classes\n",
    "\n",
    "# Connect directly to grid nodes\n",
    "nodes = [\"ws://0.0.0.0:3000/\",\n",
    "         \"ws://0.0.0.0:3001/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "#########<syft==0.2.8>#######################\n",
    "# # Dynamic FL -->\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "\n",
    "# #Static FL -->\n",
    "from syft.grid.clients.model_centric_fl_client import ModelCentricFLClient\n",
    "#############################################\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "if (Path(\"..\") / \"src\").resolve().exists():\n",
    "  sys.path.insert(0, Path(\"..\").as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing from src folder\n",
    "from src.data_splitter import Genes, Labels\n",
    "from src.data_splitter import ClientGenerator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolving base folder\n",
    "wd = Path(\".\").resolve()\n",
    "base = wd if (wd / \"data\").exists() else Path(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = base / \"data\"\n",
    "samples_path = str(data / 'gtex' / 'v8_samples.parquet')\n",
    "expressions_path = str(data / 'gtex' / 'v8_expressions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Huber(yHat, y, delta=1.):\n",
    "    return np.where(np.abs(y-yHat) < delta,.5*(y-yHat)**2 , delta*(np.abs(y-yHat)-0.5*delta))\n",
    "\n",
    "def transform_to_probas(age_intervals):\n",
    "    class_names = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n",
    "    res = []\n",
    "    for a in age_intervals:\n",
    "        non_zero_index = class_names.index(a)\n",
    "        res.append([0 if i != non_zero_index else 1 for i in range(len(class_names))])\n",
    "    return np.array(res)\n",
    "    \n",
    "def transform_to_interval(age_probas):\n",
    "    class_names = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n",
    "    return np.array(list(map(lambda p: class_names[np.argmax(p)], age_probas)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = Genes(samples_path, expressions_path, problem_type=\"classification\")\n",
    "X = genes.get_features_dataframe().values\n",
    "Y = genes.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17382, 18420), (17382, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20-29': 1320,\n",
       " '30-39': 1323,\n",
       " '40-49': 2702,\n",
       " '50-59': 5615,\n",
       " '60-69': 5821,\n",
       " '70-79': 601}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = transform_to_interval(Y)\n",
    "unique, counts = np.unique(a, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1320, 1: 1323, 2: 2702, 3: 5615, 4: 5821, 5: 601}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = [np.where(r==1)[0][0] for r in Y]\n",
    "unique, counts = np.unique(b, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pie = pd.DataFrame(columns = ['age_group','label'])\n",
    "df_pie['age_group'] = a\n",
    "df_pie['label'] = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the data below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ClientGenerator().balanced_sample_maker(X,np.asarray(df_pie['age_group'].values), no_samples_to_take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20-29': 300,\n",
       " '30-39': 300,\n",
       " '40-49': 300,\n",
       " '50-59': 300,\n",
       " '60-69': 300,\n",
       " '70-79': 300}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(res[1], return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label encoding -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(res[1])\n",
    "y = le.transform(res[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data into different shards --> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_1, X_2, y_1, y_2 = model_selection.train_test_split(res[0], y, test_size=0.5, random_state=seed, stratify=res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('uint8'), dtype('float32'), dtype('uint8'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dtype = np.float32\n",
    "y_1 = np.vstack(y_1).astype(np.uint8)\n",
    "y_2 = np.vstack(y_2).astype(np.uint8)\n",
    "X_1.dtype, y_1.dtype,X_2.dtype, y_2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 150, 1: 150, 2: 150, 3: 150, 4: 150, 5: 150}\n",
      "{0: 150, 1: 150, 2: 150, 3: 150, 4: 150, 5: 150}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_1, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(y_2, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0]\n",
    "y_1 = y_1.reshape(y_1.shape[0])\n",
    "y_2 = y_2.reshape(y_2.shape[0])\n",
    "y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.8'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sy.version.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setup config</h2>\n",
    "Init hook, connect with grid nodes, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "compute_nodes = []\n",
    "for node in nodes:\n",
    "    # For syft 0.2.8 --> replace DynamicFLClient with DataCentricFLClient\n",
    "    compute_nodes.append( DataCentricFLClient(hook, node) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Federated Worker id:h1>, <Federated Worker id:h2>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Conversion to Tensor\n",
    "\n",
    "The code below will convert GTEx data samples to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pankajverma/anaconda3/envs/pysyft_v028/lib/python3.7/site-packages/syft/frameworks/torch/hook/hook.py:560: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_tensor = hook_self.torch.native_tensor(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "shared_x1, shared_x2, shared_y1, shared_y2 = X_1, X_2, y_1, y_2\n",
    "\n",
    "# Convert numpy array to torch tensors -->\n",
    "shared_x1 = torch.from_numpy(shared_x1)\n",
    "shared_x2 = torch.from_numpy(shared_x2)\n",
    "shared_y1 = torch.from_numpy(shared_y1)\n",
    "shared_y2 = torch.from_numpy(shared_y2)\n",
    "\n",
    "shared_x1 = torch.tensor(shared_x1, dtype=torch.float32)\n",
    "shared_x2 = torch.tensor(shared_x2, dtype=torch.float32)\n",
    "shared_y1 = torch.tensor(shared_y1, dtype=torch.int64)\n",
    "shared_y2 = torch.tensor(shared_y2, dtype=torch.int64)\n",
    "\n",
    "datasets  = [shared_x1, shared_x2]\n",
    "labels = [shared_y1, shared_y2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below using centralized way (full data) --->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training loss: 0.00010541586023679214  | Training Accuracy: 0.16666666666666666\n",
      "Epoch: 1 Training loss: 0.0001053553438843075  | Training Accuracy: 0.16666666666666666\n",
      "Epoch: 2 Training loss: 0.00010528027527415622  | Training Accuracy: 0.17084362866219555\n",
      "Epoch: 3 Training loss: 0.00010519656648017586  | Training Accuracy: 0.18696317213789856\n",
      "Epoch: 4 Training loss: 0.00010510910565831519  | Training Accuracy: 0.19702317919755266\n",
      "Epoch: 5 Training loss: 0.00010502268278061579  | Training Accuracy: 0.20390634192257912\n",
      "Epoch: 6 Training loss: 0.00010493484324939897  | Training Accuracy: 0.2156724320508295\n",
      "Epoch: 7 Training loss: 0.00010484467535695549  | Training Accuracy: 0.22679138722202613\n",
      "Epoch: 8 Training loss: 0.00010475039776668421  | Training Accuracy: 0.23367454994705258\n",
      "Epoch: 9 Training loss: 0.00010465371467068778  | Training Accuracy: 0.23432168490410638\n",
      "Epoch: 10 Training loss: 0.00010455445074056061  | Training Accuracy: 0.23755735968937522\n",
      "Epoch: 11 Training loss: 0.0001044535737891017  | Training Accuracy: 0.2432050829509354\n",
      "Epoch: 12 Training loss: 0.00010435221994437952  | Training Accuracy: 0.24508765737145546\n",
      "Epoch: 13 Training loss: 0.0001042494774986848  | Training Accuracy: 0.24779385810095306\n",
      "Epoch: 14 Training loss: 0.00010414588646350685  | Training Accuracy: 0.25185315919519946\n",
      "Epoch: 15 Training loss: 0.00010404227438892024  | Training Accuracy: 0.253618072714437\n",
      "Epoch: 16 Training loss: 0.00010393909712877957  | Training Accuracy: 0.25550064713495707\n",
      "Epoch: 17 Training loss: 0.00010383584272414043  | Training Accuracy: 0.2587363219202259\n",
      "Epoch: 18 Training loss: 0.00010373369639502482  | Training Accuracy: 0.261795505353571\n",
      "Epoch: 19 Training loss: 0.00010363234956343885  | Training Accuracy: 0.26314860571831983\n",
      "Epoch: 20 Training loss: 0.00010353276302904533  | Training Accuracy: 0.2630309448170373\n",
      "Epoch: 21 Training loss: 0.00010343339390187478  | Training Accuracy: 0.2646193669843511\n",
      "Epoch: 22 Training loss: 0.00010333570792739819  | Training Accuracy: 0.2655606541946111\n",
      "Epoch: 23 Training loss: 0.00010323859703009206  | Training Accuracy: 0.26809036357218496\n",
      "Epoch: 24 Training loss: 0.00010314312720666256  | Training Accuracy: 0.2706200729497588\n",
      "Epoch: 25 Training loss: 0.00010304725763446822  | Training Accuracy: 0.2726791387222026\n",
      "Epoch: 26 Training loss: 0.00010295244003270763  | Training Accuracy: 0.27556183080362395\n",
      "Epoch: 27 Training loss: 0.00010285859725688231  | Training Accuracy: 0.2788563360395341\n",
      "Epoch: 28 Training loss: 0.00010276466331028605  | Training Accuracy: 0.28109189316390165\n",
      "Epoch: 29 Training loss: 0.00010267216705661589  | Training Accuracy: 0.2813860454171079\n",
      "Epoch: 30 Training loss: 0.00010258038614284068  | Training Accuracy: 0.28303329803506294\n",
      "Epoch: 31 Training loss: 0.00010248896289901294  | Training Accuracy: 0.28650429462289684\n",
      "Epoch: 32 Training loss: 0.0001023978271937704  | Training Accuracy: 0.28885751264854687\n",
      "Epoch: 33 Training loss: 0.00010230813619459022  | Training Accuracy: 0.2911519002235557\n",
      "Epoch: 34 Training loss: 0.00010221983379638256  | Training Accuracy: 0.292857983292152\n",
      "Epoch: 35 Training loss: 0.00010213126489899834  | Training Accuracy: 0.29391693140369457\n",
      "Epoch: 36 Training loss: 0.00010204442824626168  | Training Accuracy: 0.29744675844216967\n",
      "Epoch: 37 Training loss: 0.0001019586856427761  | Training Accuracy: 0.29868219790563594\n",
      "Epoch: 38 Training loss: 0.00010187379162877374  | Training Accuracy: 0.29921167196140724\n",
      "Epoch: 39 Training loss: 0.00010179038439965105  | Training Accuracy: 0.2993293328626897\n",
      "Epoch: 40 Training loss: 0.00010170798706214475  | Training Accuracy: 0.30262383809859983\n",
      "Epoch: 41 Training loss: 0.00010162725885105997  | Training Accuracy: 0.30262383809859983\n",
      "Epoch: 42 Training loss: 0.00010154789118840283  | Training Accuracy: 0.3032709730556536\n",
      "Epoch: 43 Training loss: 0.000101468783011786  | Training Accuracy: 0.3047417343216849\n",
      "Epoch: 44 Training loss: 0.00010139094421282588  | Training Accuracy: 0.3071537827979762\n",
      "Epoch: 45 Training loss: 0.00010131400309530254  | Training Accuracy: 0.3081539004588775\n",
      "Epoch: 46 Training loss: 0.00010123941137841455  | Training Accuracy: 0.3083303918108013\n",
      "Epoch: 47 Training loss: 0.00010116479862211788  | Training Accuracy: 0.3103894575832451\n",
      "Epoch: 48 Training loss: 0.00010109194616301366  | Training Accuracy: 0.31080127073773384\n",
      "Epoch: 49 Training loss: 0.00010101930409799619  | Training Accuracy: 0.31338981056594895\n"
     ]
    }
   ],
   "source": [
    "# Concatenate \n",
    "X = torch.cat((shared_x1, shared_x2), dim=0)\n",
    "Y = torch.cat((shared_y1, shared_y2), dim=0)\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# TODO: Define your network architecture here\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(18420, 512)\n",
    "        self.fc2 = nn.Linear(512, 64)\n",
    "        self.fc3 = nn.Linear(64, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create the network, define the criterion and optimizer\n",
    "model = Classifier()\n",
    "# criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    \n",
    "epochs = 50\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred = model(X)\n",
    "    loss = F.cross_entropy(pred, Y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # statistics\n",
    "    #prob = F.softmax(pred, dim=1)\n",
    "    top1 = torch.argmax(pred, dim=1)\n",
    "    ncorrect = torch.sum(top1 == Y)\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "    \n",
    "    epoch_acc += ncorrect.item()\n",
    "\n",
    "    epoch_loss /= Y.shape[0]\n",
    "    epoch_acc /= Y.shape[0]\n",
    "\n",
    "    print(f\"Epoch: {e}\",f\"Training loss: {epoch_loss}\", f\" | Training Accuracy: {epoch_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2 - Tagging tensors</h2>\n",
    "The code below will add a tag (of your choice) to the data that will be sent to grid nodes. This tag is important as the network will need it to retrieve this data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_input = []\n",
    "tag_label = []\n",
    "\n",
    "for i in range(len(compute_nodes)):\n",
    "    tag_input.append(datasets[i].tag(\"#X\", \"#gtex_v8\", \"#dataset\",\"#balanced\").describe(\"The input datapoints to the GTEx_V8 dataset.\"))\n",
    "    tag_label.append(labels[i].tag(\"#Y\", \"#gtex_v8\", \"#dataset\",\"#balanced\").describe(\"The input labels to the GTEx_V8 dataset.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3 - Sending our tensors to grid nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_x1 = tag_input[0].send(compute_nodes[0]) # First chunk of dataset to h1\n",
    "shared_x2 = tag_input[1].send(compute_nodes[1]) # Second chunk of dataset to h2\n",
    "\n",
    "shared_y1 = tag_label[0].send(compute_nodes[0]) # First chunk of labels to h1\n",
    "shared_y2 = tag_label[1].send(compute_nodes[1]) # Second chunk of labels to h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor pointers:  (Wrapper)>[PointerTensor | me:70361333315 -> h1:35881159115]\n",
      "\tTags: #balanced #dataset #gtex_v8 #X \n",
      "\tShape: torch.Size([900, 18420])\n",
      "\tDescription: The input datapoints to the GTEx_V8 dataset.... (Wrapper)>[PointerTensor | me:49684275468 -> h2:52723085225]\n",
      "\tTags: #balanced #dataset #gtex_v8 #X \n",
      "\tShape: torch.Size([900, 18420])\n",
      "\tDescription: The input datapoints to the GTEx_V8 dataset....\n",
      "Y tensor pointers:  (Wrapper)>[PointerTensor | me:49137384384 -> h1:4180671457]\n",
      "\tTags: #balanced #Y #gtex_v8 #dataset \n",
      "\tShape: torch.Size([900])\n",
      "\tDescription: The input labels to the GTEx_V8 dataset.... (Wrapper)>[PointerTensor | me:10561419581 -> h2:1629165329]\n",
      "\tTags: #balanced #Y #gtex_v8 #dataset \n",
      "\tShape: torch.Size([900])\n",
      "\tDescription: The input labels to the GTEx_V8 dataset....\n"
     ]
    }
   ],
   "source": [
    "print(\"X tensor pointers: \", shared_x1, shared_x2)\n",
    "print(\"Y tensor pointers: \", shared_y1, shared_y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Disconnect nodes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(compute_nodes)):\n",
    "    compute_nodes[i].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go to the following address to search available tags:\n",
    "http://0.0.0.0:5000/search-available-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysyft_v028] *",
   "language": "python",
   "name": "conda-env-pysyft_v028-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
