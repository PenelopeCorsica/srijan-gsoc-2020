{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Federated Learning - GTEx_V8 Example</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies for helper functions/classes\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from typing import NamedTuple\n",
    "import os.path as path\n",
    "import os\n",
    "import progressbar\n",
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "#keras for ML\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dropout, Input, Dense\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.utils import plot_model, normalize\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Nadam, Adadelta\n",
    "from tensorflow.keras.activations import relu, elu, sigmoid\n",
    "\n",
    "#sklearn for preprocessing the data and train-test split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "#for plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Parameter cell -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "_test_size = 0.3\n",
    "comms_round = 2\n",
    "local_epochs = 10\n",
    "CLIENTS = 2\n",
    "local_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Labels(NamedTuple):\n",
    "    '''\n",
    "    One-hot labeled data\n",
    "    '''\n",
    "    tissue: np.ndarray\n",
    "    sex: np.ndarray\n",
    "    age: np.ndarray\n",
    "    death: np.ndarray\n",
    "        \n",
    "\n",
    "class Genes:\n",
    "    '''\n",
    "    Class to load GTEX samples and gene expressions data\n",
    "    '''\n",
    "    def __init__(self, samples_path: str = '', expressions_path: str = '', problem_type: str = \"classification\"):\n",
    "        self.__set_samples(samples_path)\n",
    "        self.__set_labels(problem_type)\n",
    "        if expressions_path != '':\n",
    "            self.expressions = self.get_expressions(expressions_path)\n",
    "\n",
    "    def __set_samples(self, sample_path: str) -> pd.DataFrame:\n",
    "        self.samples: pd.DataFrame = pq.read_table(sample_path).to_pandas()\n",
    "        self.samples[\"Death\"].fillna(-1.0, inplace = True)\n",
    "        self.samples: pd.DataFrame = self.samples.set_index(\"Name\")\n",
    "        self.samples[\"Sex\"].replace([1, 2], ['male', 'female'], inplace=True)\n",
    "        self.samples[\"Death\"].replace([-1,0,1,2,3,4], ['alive/NA', 'ventilator case', '<10 min.', '<1 hr', '1-24 hr.', '>1 day'], inplace=True)\n",
    "        self.samples = self.samples[~self.samples['Death'].isin(['>1 day'])]\n",
    "        return self.samples\n",
    "\n",
    "    def __set_labels(self, problem_type: str = \"classification\") -> Labels:\n",
    "        self.labels_list = [\"Tissue\", \"Sex\", \"Age\", \"Death\"]\n",
    "        self.labels: pd.DataFrame = self.samples[self.labels_list]\n",
    "        self.drop_list = self.labels_list + [\"Subtissue\", \"Avg_age\"]\n",
    "        \n",
    "        if problem_type == \"classification\":\n",
    "            dummies_df = pd.get_dummies(self.labels[\"Age\"])\n",
    "            print(dummies_df.columns.tolist())\n",
    "            self.Y = dummies_df.values\n",
    "        \n",
    "        if problem_type == \"regression\":\n",
    "            self.Y = self.samples[\"Avg_age\"].values\n",
    "        \n",
    "        return self.Y\n",
    "\n",
    "    def sex_output(self, model):\n",
    "        return Dense(units=self.Y.sex.shape[1], activation='softmax', name='sex_output')(model)\n",
    "\n",
    "    def tissue_output(self, model):\n",
    "        return Dense(units=self.Y.tissue.shape[1], activation='softmax', name='tissue_output')(model)\n",
    "\n",
    "    def death_output(self, model):\n",
    "        return Dense(units=self.Y.death.shape[1], activation='softmax', name='death_output')(model)\n",
    "\n",
    "    def age_output(self, model):\n",
    "        '''\n",
    "        Created an output layer for the keras mode\n",
    "        :param model: keras model\n",
    "        :return: keras Dense layer\n",
    "        '''\n",
    "        return Dense(units=self.Y.age.shape[1], activation='softmax', name='age_output')(model)\n",
    "\n",
    "\n",
    "    def get_expressions(self, expressions_path: str)->pd.DataFrame:\n",
    "        '''\n",
    "        load gene expressions DataFrame\n",
    "        :param expressions_path: path to file with expressions\n",
    "        :return: pandas dataframe with expression\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if expressions_path.endswith(\".parquet\"):\n",
    "            return pq.read_table(expressions_path).to_pandas().set_index(\"Name\") \n",
    "        else:\n",
    "            separator = \",\" if expressions_path.endswith(\".csv\") else \"\\t\"\n",
    "            return pd.read_csv(expressions_path, sep=separator).set_index(\"Name\") \n",
    "\n",
    "    def prepare_data(self, normalize_expressions: bool = True)-> np.ndarray:\n",
    "        '''\n",
    "        :param normalize_expressions: if keras should normalize gene expressions\n",
    "        :return: X array to be used as input data by keras\n",
    "        '''\n",
    "        data = self.samples.join(self.expressions, on = \"Name\", how=\"inner\")\n",
    "        ji = data.columns.drop(self.drop_list)\n",
    "        x = data[ji]\n",
    "        \n",
    "        # adding one-hot-encoded tissues and sex\n",
    "        #x = pd.concat([x,pd.get_dummies(data['Tissue'], prefix='tissue'), pd.get_dummies(data['Sex'], prefix='sex')],axis=1)\n",
    "        \n",
    "        steps = [('standardization', StandardScaler()), ('normalization', MinMaxScaler())]\n",
    "        pre_processing_pipeline = Pipeline(steps)\n",
    "        transformed_data = pre_processing_pipeline.fit_transform(x)\n",
    "\n",
    "        x = transformed_data\n",
    "        \n",
    "        print('Data length', len(x))\n",
    "        \n",
    "        return x #normalize(x, axis=0) if normalize_expressions else x\n",
    "    \n",
    "    def get_features_dataframe(self, add_tissues=False):\n",
    "        data = self.samples.join(self.expressions, on = \"Name\", how=\"inner\")\n",
    "        ji = data.columns.drop(self.drop_list)\n",
    "        df = data[ji]\n",
    "        if add_tissues:\n",
    "            df = pd.concat([df,pd.get_dummies(data['Tissue'], prefix='tissue'), pd.get_dummies(data['Sex'], prefix='sex')],axis=1)\n",
    "        x = df.values\n",
    "        \n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        x_scaled = min_max_scaler.fit_transform(x)\n",
    "        df_normalized = pd.DataFrame(x_scaled, columns=df.columns, index=df.index)\n",
    "        return df_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_path = '../data/gtex/v8_samples.parquet'\n",
    "expressions_path = '../data/gtex/v8_expressions.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length 15343\n"
     ]
    }
   ],
   "source": [
    "genes = Genes(samples_path, expressions_path, problem_type=\"regression\")\n",
    "X = genes.prepare_data(True)\n",
    "Y = genes.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    Y, \n",
    "                                                    test_size=_test_size, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    #randomize the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clients\n",
    "clients = create_clients(X_train, y_train, num_clients=CLIENTS, initial='client')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['client_1', 'client_2']), (18388,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients.keys(), clients['client_1'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=256):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data, bs = local_batch_size)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['client_1', 'client_2']),\n",
       " <BatchDataset shapes: ((None, 18388), (None,)), types: (tf.float32, tf.float64)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clients_batched.keys(),clients_batched['client_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "def optimized_age_model_regression():\n",
    "    # optimized_age_model(x_train, x_val, y_train, y_val, params: dict):\n",
    "    input_layer = Input(shape=(clients['client_1'][0][0].shape[0],))\n",
    "    reg = keras.regularizers.l1_l2(l1=0.3, l2=0.3)\n",
    "    mod = Dense(1024, activation=relu)(input_layer) # 196\n",
    "    mod = Dropout(0.1)(mod) \n",
    "    mod = Dense(512, activation=relu)(mod) # 196\n",
    "    mod = Dropout(0.1)(mod)    \n",
    "    mod = Dense(64, activation=relu)(mod) #64\n",
    "    mod = Dropout(0.1)(mod)\n",
    "    \n",
    "    outputs = [Dense(1, name='age_output')(mod)] #let's try to make it simple and start with age \n",
    "    #outputs = [Dense(y_train.shape[1], activation='sigmoid', name='age_output')(mod)] #let's try to make it simple and start with age \n",
    "    loss = {'age_output': 'mse'}\n",
    "    weights={'age_output': 1.0}\n",
    "    metrics = {'age_output': ['mae', coeff_determination]}\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=outputs)\n",
    "    model.summary()\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=loss,\n",
    "              loss_weights=weights,\n",
    "              metrics=metrics,\n",
    "                 )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Huber(yHat, y, delta=1.):\n",
    "    return np.where(np.abs(y-yHat) < delta,.5*(y-yHat)**2 , delta*(np.abs(y-yHat)-0.5*delta))\n",
    "\n",
    "def transform_to_probas(age_intervals):\n",
    "    class_names = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n",
    "    res = []\n",
    "    for a in age_intervals:\n",
    "        non_zero_index = class_names.index(a)\n",
    "        res.append([0 if i != non_zero_index else 1 for i in range(len(class_names))])\n",
    "    return np.array(res)\n",
    "    \n",
    "def transform_to_interval(age_probas):\n",
    "    class_names = ['20-29', '30-39', '40-49', '50-59', '60-69', '70-79']\n",
    "    return np.array(list(map(lambda p: class_names[np.argmax(p)], age_probas)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 18388)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              18830336  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "age_output (Dense)           (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 19,388,033\n",
      "Trainable params: 19,388,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "client_2 | Round:  0\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 18388)]           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              18830336  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "age_output (Dense)           (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 19,388,033\n",
      "Trainable params: 19,388,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "21/21 [==============================] - 16s 753ms/step - loss: 640.4262 - mae: 20.0408 - coeff_determination: -2.9319\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 14s 669ms/step - loss: 317.2289 - mae: 14.1598 - coeff_determination: -0.9650\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 13s 640ms/step - loss: 265.0541 - mae: 12.9406 - coeff_determination: -0.6209\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 13s 610ms/step - loss: 227.9850 - mae: 12.0867 - coeff_determination: -0.4098\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 12s 549ms/step - loss: 209.7745 - mae: 11.6858 - coeff_determination: -0.2981\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 11s 545ms/step - loss: 192.5770 - mae: 11.2108 - coeff_determination: -0.1917\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 13s 611ms/step - loss: 181.8147 - mae: 10.8322 - coeff_determination: -0.1110\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 19s 897ms/step - loss: 167.8653 - mae: 10.4429 - coeff_determination: -0.0399\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 13s 601ms/step - loss: 161.8345 - mae: 10.3583 - coeff_determination: 0.0041\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 13s 609ms/step - loss: 143.6669 - mae: 9.7512 - coeff_determination: 0.1101\n",
      "client_1 | Round:  0\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 18388)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              18830336  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "age_output (Dense)           (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 19,388,033\n",
      "Trainable params: 19,388,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "21/21 [==============================] - 16s 768ms/step - loss: 643.1730 - mae: 20.1212 - coeff_determination: -3.1155\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 20s 961ms/step - loss: 318.2145 - mae: 14.2110 - coeff_determination: -0.9947\n",
      "Epoch 3/10\n",
      " 2/21 [=>............................] - ETA: 32s - loss: 293.1434 - mae: 13.6736 - coeff_determination: -0.8268"
     ]
    }
   ],
   "source": [
    "rmse = []\n",
    "mae = []\n",
    "r2 = []\n",
    "huber_loss = []\n",
    "    \n",
    "global_model = optimized_age_model_regression()\n",
    "    \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "    \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        print(client, '| Round: ', comm_round)\n",
    "\n",
    "        local_model = optimized_age_model_regression()\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        \n",
    "        local_model.fit(clients_batched[client], epochs=local_epochs, verbose=1)\n",
    "#         predictions = local_model.predict(X_test)\n",
    "#         test_y = y_test\n",
    "\n",
    "#         print(\"R^2\", r2_score(test_y, predictions))\n",
    "#         print(\"Mean squared error\", mean_squared_error(test_y, predictions))\n",
    "#         print(\"Mean absolute error\", mean_absolute_error(test_y, predictions))\n",
    "#         print('Huber loss', np.mean(Huber(test_y, predictions)))\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "    \n",
    "    predictions = global_model.predict(X_test)\n",
    "    test_y = y_test\n",
    "\n",
    "    print(\"R^2\", r2_score(test_y, predictions))\n",
    "    print(\"Mean squared error\", mean_squared_error(test_y, predictions))\n",
    "    print(\"Mean absolute error\", mean_absolute_error(test_y, predictions))\n",
    "    print('Huber loss', np.mean(Huber(test_y, predictions)))\n",
    "    \n",
    "    rmse.append(mean_squared_error(test_y, predictions))\n",
    "    mae.append(mean_absolute_error(test_y, predictions))\n",
    "    r2.append(r2_score(test_y, predictions))\n",
    "    huber_loss.append(np.mean(Huber(test_y, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pysyft_v028] *",
   "language": "python",
   "name": "conda-env-pysyft_v028-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
